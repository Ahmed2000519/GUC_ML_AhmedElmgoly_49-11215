{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35roXDEMudbw"
      },
      "source": [
        "# GUC Clustering Project "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIiItKbYudb2"
      },
      "source": [
        "**Objective:** \n",
        "The objective of this project teach students how to apply clustering to real data sets\n",
        "\n",
        "The projects aims to teach student: \n",
        "* Which clustering approach to use\n",
        "* Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures  \n",
        "* How to tune the parameters of each data approach\n",
        "* What is the effect of different distance functions (optional) \n",
        "* How to evaluate clustering approachs \n",
        "* How to display the output\n",
        "* What is the effect of normalizing the data \n",
        "\n",
        "Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RHS5ZoQudb4"
      },
      "source": [
        "Running this project require the following imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrueqJenudb5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.preprocessing as prep\n",
        "from sklearn.datasets import make_blobs\n",
        "from plotnine import *   \n",
        "# StandardScaler is a function to normalize the data \n",
        "# You may also check MinMaxScaler and MaxAbsScaler \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from scipy.cluster.hierarchy import linkage as la\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_clusters_2D(data,kmeans=[] ,n_clusters=0):\n",
        "    distortion=None\n",
        "    if n_clusters==0:\n",
        "        plt.scatter(data[:, 0], data[:, 1], c='b',alpha=0.5,s=20)\n",
        "    else:\n",
        "        labels = kmeans.labels_\n",
        "        centroids = kmeans.cluster_centers_\n",
        "        distortion= kmeans.inertia_\n",
        "        # Plot the data points and centroids in 2D\n",
        "        plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "        plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='black')\n",
        "        plt.xlabel('feature1')\n",
        "        plt.ylabel('feature2')\n",
        "        plt.title('Clusters of 2D data')\n",
        "        plt.show()\n",
        "    return distortion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_clusters_PCA(data, kmeans=[], n_clusters=0):\n",
        "    distortion=None\n",
        "    if n_clusters==0:\n",
        "        plt.scatter(data[:, 0], data[:, 1], c='b',alpha=0.5,s=20)\n",
        "    else:\n",
        "        # Perform clustering and plotting\n",
        "        # Perform k-means clustering on the data\n",
        "        labels = kmeans.labels_\n",
        "        centroids = kmeans.cluster_centers_\n",
        "        distortion= kmeans.inertia_\n",
        "        # Perform PCA to reduce the data to 2 dimensions\n",
        "        pca = PCA(n_components=2, random_state=0).fit(data)\n",
        "        data_2d = pca.transform(data)\n",
        "        centroids_2d = pca.transform(centroids)\n",
        "\n",
        "        # Plot the data points and centroids in 2D\n",
        "        plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "        plt.scatter(centroids_2d[:, 0], centroids_2d[:, 1], marker='x', color='black')\n",
        "        plt.xlabel('PC1')\n",
        "        plt.ylabel('PC2')\n",
        "        plt.title(f'Clusters of PCA data for k={n_clusters}')\n",
        "        plt.show()\n",
        "    return distortion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_clusters_nD(data, kmeans=[], n_clusters=0):\n",
        "    distortion = None\n",
        "    n_features = data.shape[1] # get the number of features\n",
        "    n_rows = n_cols = int(np.ceil(np.sqrt(n_features))) # get the number of rows and columns for subplots\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10)) # create a figure with subplots\n",
        "    fig.suptitle('Clusters of nD data') # set the figure title\n",
        "    for i in range(n_features): # loop over the features\n",
        "        row = i // n_cols # get the row index\n",
        "        col = i % n_cols # get the column index\n",
        "        ax = axes[row, col] # get the subplot axis\n",
        "        if n_clusters == 0: # if no clustering is performed\n",
        "            ax.scatter(data[:, i], data[:, (i+1) % n_features], c='b', alpha=0.5, s=20) # plot the data points\n",
        "        else: # if clustering is performed\n",
        "            # Perform k-means clustering on the data\n",
        "            labels = kmeans.labels_\n",
        "            centroids = kmeans.cluster_centers_\n",
        "            distortion= kmeans.inertia_\n",
        "            # Plot the data points and centroids in 2D\n",
        "            ax.scatter(data[:, i], data[:, (i+1) % n_features], c=labels, cmap='rainbow', alpha=0.5, s=20) # plot the data points with cluster colors\n",
        "            ax.scatter(centroids[:, i], centroids[:, (i+1) % n_features], marker='x', color='black') # plot the centroids with black crosses\n",
        "        ax.set_xlabel('feature' + str(i+1)) # set the x label\n",
        "        ax.set_ylabel('feature' + str((i+1) % n_features + 1)) # set the y label\n",
        "    plt.tight_layout() # adjust the layout\n",
        "    plt.show() # show the plot\n",
        "    return distortion # return the distortion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_clusters(data, algo=[]):\n",
        "    n_features = data.shape[1] # get the number of features\n",
        "    n_rows = n_cols = int(np.ceil(np.sqrt(n_features))) # get the number of rows and columns for subplots\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10)) # create a figure with subplots\n",
        "    fig.suptitle('Clusters of nD data') # set the figure title\n",
        "    for i in range(n_features): # loop over the features\n",
        "        row = i // n_cols # get the row index\n",
        "        col = i % n_cols # get the column index\n",
        "        ax = axes[row, col] # get the subplot axis\n",
        "        # Perform k-means clustering on the data\n",
        "        labels = algo.labels_\n",
        "        # Plot the data points and centroids in 2D\n",
        "        ax.scatter(data[:, i], data[:, (i+1) % n_features], c=labels, cmap='rainbow', alpha=0.5, s=20) # plot the data points with cluster colors\n",
        "        ax.set_xlabel('feature' + str(i+1)) # set the x label\n",
        "        ax.set_ylabel('feature' + str((i+1) % n_features + 1)) # set the y label\n",
        "    plt.tight_layout() # adjust the layout\n",
        "    plt.show() # show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZnIbT3Mudb6"
      },
      "source": [
        "## Multi Blob Data Set \n",
        "* The Data Set generated below has 6 cluster with varying number of users and varing densities\n",
        "* Cluster the data set below using \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeSqG318udb7",
        "outputId": "078fad92-3073-4558-b1e8-f0acd8d85d34"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [8,8]\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "n_bins = 6  \n",
        "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
        "Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
        "                                centers=centers, shuffle=False, random_state=42)\n",
        "plot_clusters_2D(Multi_blob_Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDSIGjubudb8"
      },
      "source": [
        "### Kmeans \n",
        "* Use Kmeans with different values of K to cluster the above data \n",
        "* Display the outcome of each value of K \n",
        "* Plot distortion function versus K and choose the approriate value of k \n",
        "* Plot the silhouette_score versus K and use it to choose the best K \n",
        "* Store the silhouette_score for the best K for later comparison with other clustering techniques. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ne3KmtPudb9"
      },
      "outputs": [],
      "source": [
        "best_sil=[]\n",
        "\n",
        "dist_func1=[]\n",
        "sil_scores1=[]\n",
        "K_range = range(2,11)\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k,init='k-means++',random_state=42)\n",
        "    kmeans.fit(Multi_blob_Data)\n",
        "    dist_func1.append(plot_clusters_2D(Multi_blob_Data,kmeans,k))\n",
        "    preds=kmeans.fit_predict(Multi_blob_Data)\n",
        "    sil_scores1.append(silhouette_score(Multi_blob_Data,preds))\n",
        "\n",
        "plt.figure()\n",
        "# Create a line plot of the distortion function versus the K values\n",
        "plt.plot(K_range, dist_func1, marker='o')\n",
        "# Set the x and y labels\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Distortion function\")\n",
        "plt.title(\"Distortion Function vs Number of Clusters\")\n",
        "# Show the plot\n",
        "plt.show()\n",
        "plt.figure()\n",
        "# Create a line plot of the Silhouette scores versus the K values\n",
        "plt.plot(K_range, sil_scores1, marker='o')\n",
        "# Set the x and y labels\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Silhouette scores\")\n",
        "plt.title(\"Silhouette scores vs Number of Clusters\")\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"distortion values: {dist_func1}\")\n",
        "print(f\"silhouette scores: {sil_scores1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the provided distortion values and silhouette scores, it appears that the data was clustered using the KMeans algorithm with a range of number of clusters (k) from 2 to 10.\n",
        "\n",
        "The distortion values represent the sum of squared distances between the points and their assigned cluster centers. The lower the distortion value, the closer the points are to their assigned cluster centers, indicating a better fit. The distortion values decrease as k increases, indicating that the clustering solution improves as more clusters are added. However, this does not necessarily mean that a higher k is always better, as it can lead to overfitting.\n",
        "\n",
        "The silhouette scores represent the average silhouette coefficient for all the samples. The silhouette coefficient is a measure of how well each sample fits into its assigned cluster compared to the other clusters. A score of 1 indicates that the sample fits very well into its assigned cluster, a score of 0 indicates that the sample is on the boundary between two clusters, and a score of -1 indicates that the sample was assigned to the wrong cluster. The higher the silhouette score, the better the clustering solution.\n",
        "\n",
        "In this case, the silhouette score is relatively high for k=6, indicating that this is a good clustering solution. The distortion value is also relatively low for k=6, further supporting this conclusion. Therefore, the appropriate value of k is likely to be 6.\n",
        "\n",
        "It is also worth noting that the silhouette score decreases for k>6, indicating that adding more clusters does not improve the clustering solution. This is consistent with the distortion values, which also do not decrease significantly for k>6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE7dvpOAudb9"
      },
      "source": [
        "### Hierarchal Clustering\n",
        "* Use AgglomerativeClustering function to  to cluster the above data \n",
        "* In the  AgglomerativeClustering change the following parameters \n",
        "    * Affinity (use euclidean, manhattan and cosine)\n",
        "    * Linkage( use average and single )\n",
        "    * Distance_threshold (try different)\n",
        "* For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters  \n",
        "* Find the set of parameters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
        "* Record your observation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "affinities = ['euclidean','cityblock', 'cosine']\n",
        "linkages = ['average', 'single']\n",
        "#distance_thresholds = [None, 5, 10, 15]\n",
        "sil_scores2 = []\n",
        "corres_params2 = []\n",
        "# loop over the combinations of parameters\n",
        "for affinity in affinities:\n",
        "    for linkage in linkages:\n",
        "        # plot the dendrogram\n",
        "        fig = plt.figure(figsize=(10, 5))\n",
        "        Z = la(Multi_blob_Data, method=linkage, metric=affinity)\n",
        "        plt.title('Dendrogram for AgglomerativeClustering')\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Distance')\n",
        "        dendrogram(Z)\n",
        "        plt.show()\n",
        "        distance_threshold_max=Z[:,2].max()\n",
        "        step=distance_threshold_max/8\n",
        "        array = np.arange(step,distance_threshold_max+step,step)\n",
        "        distance_thresholds = array.tolist()\n",
        "\n",
        "        for distance_threshold in distance_thresholds:\n",
        "            # create an instance of the AgglomerativeClustering class\n",
        "            ac = AgglomerativeClustering(n_clusters=None,metric=affinity, linkage=linkage, distance_threshold=distance_threshold)\n",
        "            \n",
        "            # fit the data and get the labels\n",
        "            ac.fit(Multi_blob_Data)\n",
        "            labels = ac.labels_\n",
        "            \n",
        "            # compute the silhouette score\n",
        "            if distance_threshold ==0:\n",
        "                # use the default number of clusters (2)\n",
        "                score = silhouette_score(Multi_blob_Data, labels, metric=affinity)\n",
        "            else:\n",
        "                # use the number of clusters found by the algorithm\n",
        "                score = silhouette_score(Multi_blob_Data, labels, metric=affinity)\n",
        "            \n",
        "            sil_scores2.append(score)\n",
        "            corres_params2.append({'Affinity':affinity,'Linkage':linkage,'Threshold':distance_threshold})\n",
        "            # plot the clusters\n",
        "            fig2 = plt.figure(figsize=(10, 5))\n",
        "            plt.title(f'Affinity: {affinity}, Linkage: {linkage}, Distance Threshold: {distance_threshold}, silhouette score: {score}')\n",
        "            plt.xlabel(f'Number of Clusters: {len(np.unique(labels))}')\n",
        "            plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hierarchical clustering is a method of cluster analysis that seeks to build a hierarchy of clusters by merging or splitting them based on similarity measures It can reveal the hierarchical structure of the data and handle different types of clusters\n",
        "\n",
        "You have provided a dataset of 2D points with six centers and a code snippet that performs agglomerative clustering with different parameters. Agglomerative clustering is a bottom-up approach that starts with each point as a separate cluster and then merges the closest clusters until a stopping criterion is reached\n",
        "\n",
        "The parameters that you have used are:\n",
        "\n",
        "Affinity: This is the distance metric used to measure the similarity between points. You have used euclidean, cityblock (also known as manhattan), and cosine distances\n",
        "Linkage: This is the criterion used to measure the dissimilarity between clusters. You have used average and single linkages. Average linkage computes the average distance between all pairs of points in two clusters, while single linkage computes the minimum distance between any pair of points in two clusters\n",
        "Distance_threshold: This is the threshold value for the distance between clusters. If the distance is greater than or equal to this value, the clusters will not be merged. You have used different values ranging from 0 to the maximum distance in the dendrogram\n",
        "For each combination of parameters, you have plotted the dendrogram, calculated the silhouette score, and displayed the resulting clusters. The silhouette score is a measure of how well each point fits into its assigned cluster, ranging from -1 to 1. A higher score indicates a better clustering\n",
        "\n",
        "Based on your code output, the best silhouette score is 0.64, achieved with the following parameters:\n",
        "\n",
        "Affinity: cosine\n",
        "Linkage: average\n",
        "Distance_threshold: 0.5\n",
        "This results in 4 clusters that could be seen as they are separated at the origin(0,0) into 4 quadrants\n",
        "\n",
        "Some observations that you can make from your results are:\n",
        "\n",
        "The choice of affinity, linkage, and distance_threshold can have a significant impact on the clustering outcome and the silhouette score. Different combinations can result in different numbers and shapes of clusters\n",
        "The cosine distance seems to work better than the cityblock and euclidean distances for this dataset, as it produces higher silhouette scores and more coherent clusters. \n",
        "The average linkage seems to work better than the single linkage for this dataset, as it produces higher silhouette scores and more balanced clusters. This may be because the average linkage is less sensitive to outliers and noise than the single linkage, which can create long chains of clusters\n",
        "The optimal distance_threshold depends on the desired number of clusters and the structure of the data. A lower threshold can result in more clusters, while a higher threshold can result in fewer clusters. A good way to choose the threshold is to look at the dendrogram and find the largest vertical gap that does not cross any horizontal line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myJE7vQKudb-"
      },
      "source": [
        "### DBScan\n",
        "* Use DBScan function to  to cluster the above data \n",
        "* In the  DBscan change the following parameters \n",
        "    * EPS (from 0.1 to 3)\n",
        "    * Min_samples (from 5 to 25)\n",
        "* Plot the silhouette_score versus the variation in the EPS and the min_samples\n",
        "* Plot the resulting Clusters in this case \n",
        "* Find the set of parameters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
        "* Record your observations and comments "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiQtpAt5udb_"
      },
      "outputs": [],
      "source": [
        "# define the range of parameters\n",
        "eps_values = np.arange(0.4, 2.1, 0.2)\n",
        "min_samples_values = np.arange(5, 26, 2)\n",
        "\n",
        "# initialize an empty list to store the silhouette scores\n",
        "sil_scores3 = []\n",
        "corres_params3  = []\n",
        "\n",
        "# loop over the combinations of parameters\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        # create an instance of the DBSCAN class\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        \n",
        "        # fit the data and get the labels\n",
        "        dbscan.fit(Multi_blob_Data)\n",
        "        labels = dbscan.labels_\n",
        "        \n",
        "        # compute the silhouette score\n",
        "        # ignore the noise points (labelled as -1) for the score calculation\n",
        "        score = silhouette_score(Multi_blob_Data, labels, metric='euclidean')\n",
        "        \n",
        "        # append the score to the list\n",
        "        sil_scores3.append(score)\n",
        "        corres_params3.append( {\"eps\":eps, \"min_samples\":min_samples} )\n",
        "        \n",
        "        # plot the clusters\n",
        "        fig = plt.figure(figsize=(10, 5))\n",
        "        plt.title(f'EPS: {eps}, Min_samples: {min_samples}, silhouette score: {score}')\n",
        "        plt.xlabel(f'Number of Clusters: {len(np.unique(labels)) - 1}') # subtract 1 to exclude the noise cluster\n",
        "        plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the code output, the best silhouette score is 0.66, achieved with the following parameters:\n",
        "\n",
        "EPS: 0.9\n",
        "Min_samples: 5\n",
        "This results in six clusters that match the original centers of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip16g1QFudb_"
      },
      "source": [
        "### Gaussian Mixture\n",
        "* Use GaussianMixture function to cluster the above data \n",
        "* In GMM change the covariance_type and check the difference in the resulting proabability fit \n",
        "* Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "# Define a function to display the clusters\n",
        "def display_cluster(X, y_pred, title):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_gmm_contours(gmm, X, title):\n",
        "    # Create a grid of points to evaluate the GMM\n",
        "    x = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
        "    y = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    # Compute the log probability density of each point under each component\n",
        "    z = gmm.score_samples(X_grid)\n",
        "    z = z.reshape(xx.shape)\n",
        "    # Compute the log probability density of each point under the total mixture\n",
        "    log_prob_total = np.exp(z)\n",
        "\n",
        "    # Plot the contours of the components and the total mixture\n",
        "    plt.contour(xx, yy, log_prob_total, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=14, cmap='viridis')\n",
        "    plt.contour(xx, yy, log_prob_total, levels=14, colors='k')\n",
        "    display_cluster(X, gmm.predict(X), title)\n",
        "# Define a list of covariance types to try\n",
        "cov_types = ['full','spherical', 'diag', 'tied']\n",
        "\n",
        "# Loop over the covariance types and fit a GMM for each one\n",
        "for cov_type in cov_types:\n",
        "    gmm = GaussianMixture(n_components=6, covariance_type=cov_type, random_state=42)\n",
        "    gmm.fit(Multi_blob_Data)\n",
        "    y_pred = gmm.predict(Multi_blob_Data)\n",
        "    # Display the clusters\n",
        "    display_cluster(Multi_blob_Data, y_pred, f'GMM with 6 components and {cov_type} covariance')\n",
        "    plot_gmm_contours(gmm, Multi_blob_Data, f'GMM with 6 components and {cov_type} covariance')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m92lZkkyudb_"
      },
      "source": [
        "## iris data set \n",
        "The iris data set is test data set that is part of the Sklearn module \n",
        "which contains 150 records each with 4 features. All the features are represented by real numbers \n",
        "\n",
        "The data represents three classes \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QaCWyyCudcA",
        "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_data_raw = load_iris()\n",
        "iris_data_raw.target[[10, 25, 50]]\n",
        "#array([0, 0, 1])\n",
        "list(iris_data_raw.target_names)\n",
        "['setosa', 'versicolor', 'virginica']\n",
        "iris_data=iris_data_raw['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyoCVfyMudcA"
      },
      "source": [
        "* Repeat all the above clustering approaches and steps on the above data \n",
        "* Normalize the data then repeat all the above steps \n",
        "* Compare between the different clustering approaches "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## k-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dist_func_iris=[]\n",
        "sil_scores_iris=[]\n",
        "K_range = range(2,11)\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k,init='k-means++',random_state=42)\n",
        "    kmeans.fit(iris_data)\n",
        "    plot_clusters_PCA(iris_data,kmeans,k)\n",
        "    dist_func_iris.append(plot_clusters_nD(iris_data,kmeans,k))\n",
        "    preds=kmeans.fit_predict(iris_data)\n",
        "    sil_scores_iris.append(silhouette_score(iris_data,preds))\n",
        "\n",
        "plt.figure()\n",
        "# Create a line plot of the distortion function versus the K values\n",
        "plt.plot(K_range, dist_func_iris, marker='o')\n",
        "# Set the x and y labels\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Distortion function\")\n",
        "plt.title(\"Distortion Function vs Number of Clusters\")\n",
        "# Show the plot\n",
        "plt.show()\n",
        "plt.figure()\n",
        "# Create a line plot of the Silhouette scores versus the K values\n",
        "plt.plot(K_range, sil_scores_iris, marker='o')\n",
        "# Set the x and y labels\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Silhouette scores\")\n",
        "plt.title(\"Silhouette scores vs Number of Clusters\")\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# hierarchical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "affinities = ['euclidean','cityblock', 'cosine']\n",
        "linkages = ['average', 'single']\n",
        "sil_scores_iris2 = []\n",
        "corres_params_iris2 = []\n",
        "# loop over the combinations of parameters\n",
        "for affinity in affinities:\n",
        "    for linkage in linkages:\n",
        "        # plot the dendrogram\n",
        "        fig = plt.figure(figsize=(10, 5))\n",
        "        Z = la(iris_data, method=linkage, metric=affinity)\n",
        "        plt.title('Dendrogram for AgglomerativeClustering')\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Distance')\n",
        "        dendrogram(Z)\n",
        "        plt.show()\n",
        "        distance_threshold_max=Z[:,2].max()\n",
        "        step=distance_threshold_max/8\n",
        "        array = np.arange(step,distance_threshold_max+step,step)\n",
        "        distance_thresholds = array.tolist()\n",
        "\n",
        "        for distance_threshold in distance_thresholds:\n",
        "            # create an instance of the AgglomerativeClustering class\n",
        "            ac = AgglomerativeClustering(n_clusters=None,metric=affinity, linkage=linkage, distance_threshold=distance_threshold)\n",
        "            \n",
        "            # fit the data and get the labels\n",
        "            ac.fit(iris_data)\n",
        "            labels = ac.labels_\n",
        "            \n",
        "            # compute the silhouette score\n",
        "            if distance_threshold ==0:\n",
        "                # use the default number of clusters (2)\n",
        "                score = silhouette_score(iris_data, labels, metric=affinity)\n",
        "            else:\n",
        "                # use the number of clusters found by the algorithm\n",
        "                score = silhouette_score(iris_data, labels, metric=affinity)\n",
        "            \n",
        "            sil_scores_iris2.append(score)\n",
        "            corres_params_iris2.append({'Affinity':affinity,'Linkage':linkage,'Threshold':distance_threshold})\n",
        "            \n",
        "            # Perform PCA to reduce the data to 2 dimensions\n",
        "            pca = PCA(n_components=2, random_state=0).fit(iris_data)\n",
        "            data_2d = pca.transform(iris_data)\n",
        "            \n",
        "            # plot the clusters\n",
        "            fig2 = plt.figure(figsize=(10, 5))\n",
        "            plt.title(f'Affinity: {affinity}, Linkage: {linkage}, Distance Threshold: {distance_threshold}, silhouette score: {score}')\n",
        "            plt.xlabel(f'Number of Clusters: {len(np.unique(labels))}')\n",
        "            plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "            plt.show()\n",
        "            display_clusters(iris_data,ac)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#DBscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the range of parameters\n",
        "eps_values = np.arange(0.4, 1.6, 0.1)\n",
        "min_samples_values = np.arange(5, 21, 1)\n",
        "\n",
        "# initialize an empty list to store the silhouette scores\n",
        "sil_scores_iris3 = []\n",
        "corres_params_iris3  = []\n",
        "\n",
        "# loop over the combinations of parameters\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        # create an instance of the DBSCAN class\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        \n",
        "        # fit the data and get the labels\n",
        "        dbscan.fit(iris_data)\n",
        "        labels = dbscan.labels_\n",
        "        \n",
        "        # compute the silhouette score\n",
        "        # ignore the noise points (labelled as -1) for the score calculation\n",
        "        score = silhouette_score(iris_data, labels)\n",
        "        \n",
        "        # append the score to the list\n",
        "        sil_scores_iris3.append(score)\n",
        "        corres_params_iris3.append( {\"eps\":eps, \"min_samples\":min_samples} )\n",
        "        # Perform PCA to reduce the data to 2 dimensions\n",
        "        pca = PCA(n_components=2, random_state=0).fit(iris_data)\n",
        "        data_2d = pca.transform(iris_data)\n",
        "        # plot the clusters\n",
        "        fig = plt.figure(figsize=(10, 5))\n",
        "        plt.title(f'EPS: {eps}, Min_samples: {min_samples}, silhouette score: {score}')\n",
        "        plt.xlabel(f'Number of Clusters: {len(np.unique(labels)) - 1}') # subtract 1 to exclude the noise cluster\n",
        "        plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "        plt.show()\n",
        "        display_clusters(iris_data,dbscan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = iris_data\n",
        "# Apply PCA to reduce the dimensionality to 2\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "# Define a list of covariance types to try\n",
        "cov_types = ['full','spherical', 'diag', 'tied']\n",
        "\n",
        "# Loop over the covariance types and fit a GMM for each one\n",
        "for cov_type in cov_types:\n",
        "    gmm = GaussianMixture(n_components=3, covariance_type=cov_type, random_state=42)\n",
        "    gmm.fit(X_pca)\n",
        "    y_pred = gmm.predict(X_pca)\n",
        "    # Display the clusters\n",
        "    display_cluster(X_pca, y_pred, f'GMM with 3 components and {cov_type} covariance')\n",
        "    plot_gmm_contours(gmm, X_pca, f'GMM with 3 components and {cov_type} covariance')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2oBmWT2udcA"
      },
      "source": [
        "## Customer dataset\n",
        "Repeat all the above on the customer data set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## k-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_points=pd.read_csv(\"Customer data.csv\",index_col=0)\n",
        "data_points=data_points.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dist_func_data=[]\n",
        "sil_scores_data=[]\n",
        "K_range = range(2,11)\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k,init='k-means++',random_state=42)\n",
        "    kmeans.fit(data_points)\n",
        "    plot_clusters_PCA(data_points,kmeans,k)\n",
        "    dist_func_data.append(plot_clusters_nD(data_points,kmeans,k))\n",
        "    preds=kmeans.fit_predict(data_points)\n",
        "    sil_scores_data.append(silhouette_score(data_points,preds,metric='euclidean'))\n",
        "\n",
        "plt.figure()\n",
        "# Create a line plot of the distortion function versus the K values\n",
        "plt.plot(K_range, dist_func_data, marker='o')\n",
        "# Set the x and y labels\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Distortion function\")\n",
        "plt.title(\"Distortion Function vs Number of Clusters\")\n",
        "# Show the plot\n",
        "plt.show()\n",
        "plt.figure()\n",
        "# Create a line plot of the Silhouette scores versus the K values\n",
        "plt.plot(K_range, sil_scores_data, marker='o')\n",
        "# Set the x and y labels\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Silhouette scores\")\n",
        "plt.title(\"Silhouette scores vs Number of Clusters\")\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# hierarchical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "affinities = ['euclidean','cityblock', 'cosine']\n",
        "linkages = ['average', 'single']\n",
        "sil_scores_data2 = []\n",
        "corres_params_data2 = []\n",
        "# loop over the combinations of parameters\n",
        "for affinity in affinities:\n",
        "    for linkage in linkages:\n",
        "        # plot the dendrogram\n",
        "        fig = plt.figure(figsize=(10, 5))\n",
        "        Z = la(data_points, method=linkage, metric=affinity)\n",
        "        plt.title('Dendrogram for AgglomerativeClustering')\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Distance')\n",
        "        dendrogram(Z)\n",
        "        plt.show()\n",
        "        distance_threshold_max=Z[:,2].max()\n",
        "        step=distance_threshold_max/8\n",
        "        array = np.arange(step,distance_threshold_max+step,step)\n",
        "        distance_thresholds = array.tolist()\n",
        "\n",
        "        for distance_threshold in distance_thresholds:\n",
        "            # create an instance of the AgglomerativeClustering class\n",
        "            ac = AgglomerativeClustering(n_clusters=None,metric=affinity, linkage=linkage, distance_threshold=distance_threshold)\n",
        "            \n",
        "            # fit the data and get the labels\n",
        "            ac.fit(data_points)\n",
        "            labels = ac.labels_\n",
        "            \n",
        "            # compute the silhouette score\n",
        "            if distance_threshold ==0:\n",
        "                # use the default number of clusters (2)\n",
        "                score = silhouette_score(data_points, labels, metric=affinity)\n",
        "            else:\n",
        "                # use the number of clusters found by the algorithm\n",
        "                score = silhouette_score(data_points, labels, metric=affinity)\n",
        "            \n",
        "            sil_scores_data2.append(score)\n",
        "            corres_params_data2.append({'Affinity':affinity,'Linkage':linkage,'Threshold':distance_threshold})\n",
        "            \n",
        "            # Perform PCA to reduce the data to 2 dimensions\n",
        "            pca = PCA(n_components=2, random_state=0).fit(data_points)\n",
        "            data_2d = pca.transform(data_points)\n",
        "            \n",
        "            # plot the clusters\n",
        "            fig2 = plt.figure(figsize=(10, 5))\n",
        "            plt.title(f'Affinity: {affinity}, Linkage: {linkage}, Distance Threshold: {distance_threshold}, silhouette score: {score}')\n",
        "            plt.xlabel(f'Number of Clusters: {len(np.unique(labels))}')\n",
        "            plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "            plt.show()\n",
        "            display_clusters(data_points,ac)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the range of parameters\n",
        "eps_values = np.arange(700, 1500, 100)\n",
        "min_samples_values = np.arange(2, 21, 2)\n",
        "\n",
        "# initialize an empty list to store the silhouette scores\n",
        "sil_scores_data3 = []\n",
        "corres_params_data3  = []\n",
        "\n",
        "# loop over the combinations of parameters\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        # create an instance of the DBSCAN class\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        \n",
        "        # fit the data and get the labels\n",
        "        dbscan.fit(data_points)\n",
        "        labels = dbscan.labels_\n",
        "        \n",
        "        # compute the silhouette score\n",
        "        # ignore the noise points (labelled as -1) for the score calculation\n",
        "        score = silhouette_score(data_points, labels)\n",
        "        \n",
        "        # append the score to the list\n",
        "        sil_scores_data3.append(score)\n",
        "        corres_params_data3.append( {\"eps\":eps, \"min_samples\":min_samples} )\n",
        "        # Perform PCA to reduce the data to 2 dimensions\n",
        "        pca = PCA(n_components=2, random_state=0).fit(data_points)\n",
        "        data_2d = pca.transform(data_points)\n",
        "        # plot the clusters\n",
        "        fig = plt.figure(figsize=(10, 5))\n",
        "        plt.title(f'EPS: {eps}, Min_samples: {min_samples}, silhouette score: {score}')\n",
        "        plt.xlabel(f'Number of Clusters: {len(np.unique(labels)) - 1}') # subtract 1 to exclude the noise cluster\n",
        "        plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "        plt.show()\n",
        "        display_clusters(data_points,dbscan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = data_points\n",
        "\n",
        "# Apply PCA to reduce the dimensionality to 2\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Define a list of covariance types to try\n",
        "cov_types = ['full','spherical', 'diag', 'tied']\n",
        "\n",
        "# Loop over the covariance types and fit a GMM for each one\n",
        "for cov_type in cov_types:\n",
        "    gmm = GaussianMixture(n_components=5, covariance_type=cov_type, random_state=42)\n",
        "    gmm.fit(X_pca)\n",
        "    y_pred = gmm.predict(X_pca)\n",
        "    # Display the clusters\n",
        "    display_cluster(X_pca, y_pred, f'GMM with 5 components and {cov_type} covariance')\n",
        "    plot_gmm_contours(gmm, X_pca, f'GMM with 5 components and {cov_type} covariance')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Clustering Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
