{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running this project require the following imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.preprocessing as prep\n",
        "from sklearn.datasets import make_blobs\n",
        "from plotnine import *   \n",
        "# StandardScaler is a function to normalize the data \n",
        "# You may also check MinMaxScaler and MaxAbsScaler \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from scipy.cluster.hierarchy import linkage as la\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_clusters_PCA(data, kmeans=[], n_clusters=0):\n",
        "    distortion=None\n",
        "    if n_clusters==0:\n",
        "        plt.scatter(data[:, 0], data[:, 1], c='b',alpha=0.5,s=20)\n",
        "    else:\n",
        "        # Perform clustering and plotting\n",
        "        # Perform k-means clustering on the data\n",
        "        labels = kmeans.labels_\n",
        "        centroids = kmeans.cluster_centers_\n",
        "        distortion= kmeans.inertia_\n",
        "        # Perform PCA to reduce the data to 2 dimensions\n",
        "        pca = PCA(n_components=2, random_state=0).fit(data)\n",
        "        data_2d = pca.transform(data)\n",
        "        centroids_2d = pca.transform(centroids)\n",
        "\n",
        "        # Plot the data points and centroids in 2D\n",
        "        plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "        plt.scatter(centroids_2d[:, 0], centroids_2d[:, 1], marker='x', color='black')\n",
        "        plt.xlabel('PC1')\n",
        "        plt.ylabel('PC2')\n",
        "        plt.title(f'Clusters of PCA data for k={n_clusters}')\n",
        "        plt.show()\n",
        "    return distortion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_clusters_nD(data, kmeans=[], n_clusters=0):\n",
        "    distortion = None\n",
        "    n_features = data.shape[1] # get the number of features\n",
        "    n_rows = n_cols = int(np.ceil(np.sqrt(n_features))) # get the number of rows and columns for subplots\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10)) # create a figure with subplots\n",
        "    fig.suptitle('Clusters of nD data') # set the figure title\n",
        "    for i in range(n_features): # loop over the features\n",
        "        row = i // n_cols # get the row index\n",
        "        col = i % n_cols # get the column index\n",
        "        ax = axes[row, col] # get the subplot axis\n",
        "        if n_clusters == 0: # if no clustering is performed\n",
        "            ax.scatter(data[:, i], data[:, (i+1) % n_features], c='b', alpha=0.5, s=20) # plot the data points\n",
        "        else: # if clustering is performed\n",
        "            # Perform k-means clustering on the data\n",
        "            labels = kmeans.labels_\n",
        "            centroids = kmeans.cluster_centers_\n",
        "            distortion= kmeans.inertia_\n",
        "            # Plot the data points and centroids in 2D\n",
        "            ax.scatter(data[:, i], data[:, (i+1) % n_features], c=labels, cmap='rainbow', alpha=0.5, s=20) # plot the data points with cluster colors\n",
        "            ax.scatter(centroids[:, i], centroids[:, (i+1) % n_features], marker='x', color='black') # plot the centroids with black crosses\n",
        "        ax.set_xlabel('feature' + str(i+1)) # set the x label\n",
        "        ax.set_ylabel('feature' + str((i+1) % n_features + 1)) # set the y label\n",
        "    plt.tight_layout() # adjust the layout\n",
        "    plt.show() # show the plot\n",
        "    return distortion # return the distortion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_clusters(data, algo=[]):\n",
        "    n_features = data.shape[1] # get the number of features\n",
        "    n_rows = n_cols = int(np.ceil(np.sqrt(n_features))) # get the number of rows and columns for subplots\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10)) # create a figure with subplots\n",
        "    fig.suptitle('Clusters of nD data') # set the figure title\n",
        "    for i in range(n_features): # loop over the features\n",
        "        row = i // n_cols # get the row index\n",
        "        col = i % n_cols # get the column index\n",
        "        ax = axes[row, col] # get the subplot axis\n",
        "        # Perform k-means clustering on the data\n",
        "        labels = algo.labels_\n",
        "        # Plot the data points and centroids in 2D\n",
        "        ax.scatter(data[:, i], data[:, (i+1) % n_features], c=labels, cmap='rainbow', alpha=0.5, s=20) # plot the data points with cluster colors\n",
        "        ax.set_xlabel('feature' + str(i+1)) # set the x label\n",
        "        ax.set_ylabel('feature' + str((i+1) % n_features + 1)) # set the y label\n",
        "    plt.tight_layout() # adjust the layout\n",
        "    plt.show() # show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian Mixture\n",
        "* Use GaussianMixture function to cluster the above data \n",
        "* In GMM change the covariance_type and check the difference in the resulting proabability fit \n",
        "* Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## iris data set \n",
        "The iris data set is test data set that is part of the Sklearn module \n",
        "which contains 150 records each with 4 features. All the features are represented by real numbers \n",
        "\n",
        "The data represents three classes \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_data_raw = load_iris()\n",
        "iris_data_raw.target[[10, 25, 50]]\n",
        "#array([0, 0, 1])\n",
        "list(iris_data_raw.target_names)\n",
        "['setosa', 'versicolor', 'virginica']\n",
        "iris_data=iris_data_raw['data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean = np.mean(iris_data,axis=0)\n",
        "std = np.std(iris_data,axis=0)\n",
        "min_val = np.min(iris_data,axis=0)\n",
        "max_val = np.max(iris_data,axis=0)\n",
        "norm = np.linalg.norm(iris_data,axis=0)\n",
        "\n",
        "standard_iris = (iris_data - mean) / std\n",
        "minMax_iris = (iris_data - min_val) / (max_val - min_val)\n",
        "l2_iris = iris_data / norm\n",
        "max_abs_iris = iris_data / np.abs(max_val)\n",
        "\n",
        "iris_list = [{\"data\":standard_iris,\"desc\":\"standardized_data\"},\\\n",
        "    {\"data\":minMax_iris,\"desc\":\"min_max_data\"},\\\n",
        "        {\"data\":l2_iris,\"desc\":\"l2_data\"},\\\n",
        "        {\"data\":max_abs_iris,\"desc\":\"max_abs_data\"}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Repeat all the above clustering approaches and steps on the above data \n",
        "* Normalize the data then repeat all the above steps \n",
        "* Compare between the different clustering approaches "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#k-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "K_range = range(2,11)\n",
        "for i in iris_list:\n",
        "    dist_func_iris=[]\n",
        "    sil_scores_iris=[]\n",
        "    data = i[\"data\"]\n",
        "    desc = i[\"desc\"]\n",
        "    print(f\"graphs for {desc}\")\n",
        "    for k in K_range:\n",
        "        kmeans = KMeans(n_clusters=k,init='k-means++',random_state=42)\n",
        "        kmeans.fit(iris_data)\n",
        "        plot_clusters_PCA(iris_data,kmeans,k)\n",
        "        dist_func_iris.append(plot_clusters_nD(iris_data,kmeans,k))\n",
        "        preds=kmeans.fit_predict(iris_data)\n",
        "        sil_scores_iris.append(silhouette_score(iris_data,preds))\n",
        "\n",
        "    plt.figure()\n",
        "    # Create a line plot of the distortion function versus the K values\n",
        "    plt.plot(K_range, dist_func_iris, marker='o')\n",
        "    # Set the x and y labels\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Distortion function\")\n",
        "    plt.title(\"Distortion Function vs Number of Clusters\")\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "    # Create a line plot of the Silhouette scores versus the K values\n",
        "    plt.plot(K_range, sil_scores_iris, marker='o')\n",
        "    # Set the x and y labels\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Silhouette scores\")\n",
        "    plt.title(\"Silhouette scores vs Number of Clusters\")\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#hierarchical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "affinities = ['euclidean','cityblock', 'cosine']\n",
        "linkages = ['average', 'single']\n",
        "\n",
        "\n",
        "sil_scores_iris2 = [[],[],[],[]]\n",
        "corres_params_iris2 = [[],[],[],[]]\n",
        "counter = 0\n",
        "for i in iris_list:\n",
        "    data = i[\"data\"]\n",
        "    desc = i[\"desc\"]\n",
        "    print(f\"graphs for {desc}\")\n",
        "    # loop over the combinations of parameters\n",
        "    for affinity in affinities:\n",
        "        for linkage in linkages:\n",
        "            # plot the dendrogram\n",
        "            fig = plt.figure(figsize=(10, 5))\n",
        "            Z = la(iris_data, method=linkage, metric=affinity)\n",
        "            plt.title('Dendrogram for AgglomerativeClustering')\n",
        "            plt.xlabel('Sample Index')\n",
        "            plt.ylabel('Distance')\n",
        "            dendrogram(Z)\n",
        "            plt.show()\n",
        "            distance_threshold_max=Z[:,2].max()\n",
        "            step=distance_threshold_max/8\n",
        "            array = np.arange(step,distance_threshold_max+step,step)\n",
        "            distance_thresholds = array.tolist()\n",
        "\n",
        "            for distance_threshold in distance_thresholds:\n",
        "                # create an instance of the AgglomerativeClustering class\n",
        "                ac = AgglomerativeClustering(n_clusters=None,metric=affinity, linkage=linkage, distance_threshold=distance_threshold)\n",
        "                \n",
        "                # fit the data and get the labels\n",
        "                ac.fit(iris_data)\n",
        "                labels = ac.labels_\n",
        "                \n",
        "                # compute the silhouette score\n",
        "                if distance_threshold ==0:\n",
        "                    # use the default number of clusters (2)\n",
        "                    score = silhouette_score(iris_data, labels, metric=affinity)\n",
        "                else:\n",
        "                    # use the number of clusters found by the algorithm\n",
        "                    score = silhouette_score(iris_data, labels, metric=affinity)\n",
        "                \n",
        "                sil_scores_iris2[counter].append(score)\n",
        "                corres_params_iris2[counter].append({'Affinity':affinity,'Linkage':linkage,'Threshold':distance_threshold})\n",
        "                \n",
        "                # Perform PCA to reduce the data to 2 dimensions\n",
        "                pca = PCA(n_components=2, random_state=0).fit(iris_data)\n",
        "                data_2d = pca.transform(iris_data)\n",
        "                \n",
        "                # plot the clusters\n",
        "                fig2 = plt.figure(figsize=(10, 5))\n",
        "                plt.title(f'Affinity: {affinity}, Linkage: {linkage}, Distance Threshold: {distance_threshold}, silhouette score: {score}')\n",
        "                plt.xlabel(f'Number of Clusters: {len(np.unique(labels))}')\n",
        "                plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "                plt.show()\n",
        "                display_clusters(iris_data,ac)\n",
        "    counter+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#DBscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the range of parameters\n",
        "eps_values = np.arange(0.4, 1.6, 0.1)\n",
        "min_samples_values = np.arange(5, 21, 1)\n",
        "\n",
        "# initialize an empty list to store the silhouette scores\n",
        "sil_scores_iris3 = [[],[],[],[]]\n",
        "corres_params_iris3  = [[],[],[],[]]\n",
        "counter = 0\n",
        "for i in iris_list:\n",
        "    data = i[\"data\"]\n",
        "    desc = i[\"desc\"]\n",
        "    print(f\"graphs for {desc}\")\n",
        "    # loop over the combinations of parameters\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            # create an instance of the DBSCAN class\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            \n",
        "            # fit the data and get the labels\n",
        "            dbscan.fit(iris_data)\n",
        "            labels = dbscan.labels_\n",
        "            \n",
        "            # compute the silhouette score\n",
        "            # ignore the noise points (labelled as -1) for the score calculation\n",
        "            score = silhouette_score(iris_data, labels)\n",
        "            \n",
        "            # append the score to the list\n",
        "            sil_scores_iris3.append(score)\n",
        "            corres_params_iris3.append( {\"eps\":eps, \"min_samples\":min_samples} )\n",
        "            # Perform PCA to reduce the data to 2 dimensions\n",
        "            pca = PCA(n_components=2, random_state=0).fit(iris_data)\n",
        "            data_2d = pca.transform(iris_data)\n",
        "            # plot the clusters\n",
        "            fig = plt.figure(figsize=(10, 5))\n",
        "            plt.title(f'EPS: {eps}, Min_samples: {min_samples}, silhouette score: {score}')\n",
        "            plt.xlabel(f'Number of Clusters: {len(np.unique(labels)) - 1}') # subtract 1 to exclude the noise cluster\n",
        "            plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "            plt.show()\n",
        "            display_clusters(iris_data,dbscan)\n",
        "    counter+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.colors import LogNorm\n",
        "# Define a function to display the clusters\n",
        "def display_cluster(X, y_pred, title):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_gmm_contours(gmm, X, title):\n",
        "    # Create a grid of points to evaluate the GMM\n",
        "    x = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
        "    y = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    # Compute the log probability density of each point under each component\n",
        "    z = gmm.score_samples(X_grid)\n",
        "    z = z.reshape(xx.shape)\n",
        "    # Compute the log probability density of each point under the total mixture\n",
        "    log_prob_total = np.exp(z)\n",
        "    \n",
        "    # Plot the contours of the components and the total mixture\n",
        "    plt.contour(xx, yy, log_prob_total, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=14, cmap='viridis')\n",
        "    plt.contour(xx, yy, log_prob_total, levels=14, colors='k')\n",
        "    display_cluster(X, gmm.predict(X), title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = 0\n",
        "for X in iris_list:\n",
        "    data = i[\"data\"]\n",
        "    desc = i[\"desc\"]\n",
        "    print(f\"graphs for {desc}\")\n",
        "    # Apply PCA to reduce the dimensionality to 2\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X['data'])\n",
        "    # Define a list of covariance types to try\n",
        "    cov_types = ['full','spherical', 'diag', 'tied']\n",
        "\n",
        "    # Loop over the covariance types and fit a GMM for each one\n",
        "    for cov_type in cov_types:\n",
        "        gmm = GaussianMixture(n_components=3, covariance_type=cov_type, random_state=42)\n",
        "        gmm.fit(X_pca)\n",
        "        y_pred = gmm.predict(X_pca)\n",
        "        # Display the clusters\n",
        "        display_cluster(X_pca, y_pred, f'GMM with 3 components and {cov_type} covariance')\n",
        "        plot_gmm_contours(gmm, X_pca, f'GMM with 3 components and {cov_type} covariance')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customer dataset\n",
        "Repeat all the above on the customer data set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## k-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_points=pd.read_csv(\"Customer data.csv\",index_col=0)\n",
        "data_points=data_points.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean = np.mean(data_points,axis=0)\n",
        "std = np.std(data_points,axis=0)\n",
        "min_val = np.min(data_points,axis=0)\n",
        "max_val = np.max(data_points,axis=0)\n",
        "norm = np.linalg.norm(data_points,axis=0)\n",
        "\n",
        "standard_data_points = (data_points - mean) / std\n",
        "minMax_data_points = (data_points - min_val) / (max_val - min_val)\n",
        "l2_data_points = data_points / norm\n",
        "max_abs_data_points = data_points / np.abs(max_val)\n",
        "\n",
        "data_points_list = [{\"data\":standard_data_points,\"desc\":\"standardized_data\"},\\\n",
        "    {\"data\":minMax_data_points,\"desc\":\"min_max_data\"},\\\n",
        "        {\"data\":l2_data_points,\"desc\":\"l2_data\"},\\\n",
        "        {\"data\":max_abs_data_points,\"desc\":\"max_abs_data\"}]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "K_range = range(2,11)\n",
        "for i in data_points_list:\n",
        "    dist_func_data=[]\n",
        "    sil_scores_data=[]\n",
        "    data = i[\"data\"]\n",
        "    desc = i[\"desc\"]\n",
        "    print(f\"graphs for {desc}\")\n",
        "    for k in K_range:\n",
        "        kmeans = KMeans(n_clusters=k,init='k-means++',random_state=42)\n",
        "        kmeans.fit(data)\n",
        "        plot_clusters_PCA(data,kmeans,k)\n",
        "        dist_func_data.append(plot_clusters_nD(data,kmeans,k))\n",
        "        preds=kmeans.fit_predict(data)\n",
        "        sil_scores_data.append(silhouette_score(data,preds,metric='euclidean'))\n",
        "\n",
        "    plt.figure()\n",
        "    # Create a line plot of the distortion function versus the K values\n",
        "    plt.plot(K_range, dist_func_data, marker='o')\n",
        "    # Set the x and y labels\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Distortion function\")\n",
        "    plt.title(\"Distortion Function vs Number of Clusters\")\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "    # Create a line plot of the Silhouette scores versus the K values\n",
        "    plt.plot(K_range, sil_scores_data, marker='o')\n",
        "    # Set the x and y labels\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Silhouette scores\")\n",
        "    plt.title(\"Silhouette scores vs Number of Clusters\")\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# hierarchical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "affinities = ['euclidean','cityblock', 'cosine']\n",
        "linkages = ['average', 'single']\n",
        "sil_scores_data2 = [[],[],[],[]]\n",
        "corres_params_data2 = [[],[],[],[]]\n",
        "counter = 0\n",
        "for i in data_points_list:\n",
        "    data = i[\"data\"]\n",
        "    desc = i[\"desc\"]\n",
        "    print(f\"graphs for {desc}\")\n",
        "    # loop over the combinations of parameters\n",
        "    for affinity in affinities:\n",
        "        for linkage in linkages:\n",
        "            # plot the dendrogram\n",
        "            fig = plt.figure(figsize=(10, 5))\n",
        "            Z = la(data, method=linkage, metric=affinity)\n",
        "            plt.title('Dendrogram for AgglomerativeClustering')\n",
        "            plt.xlabel('Sample Index')\n",
        "            plt.ylabel('Distance')\n",
        "            dendrogram(Z)\n",
        "            plt.show()\n",
        "            distance_threshold_max=Z[:,2].max()\n",
        "            step=distance_threshold_max/8\n",
        "            array = np.arange(step,distance_threshold_max+step,step)\n",
        "            distance_thresholds = array.tolist()\n",
        "\n",
        "            for distance_threshold in distance_thresholds:\n",
        "                # create an instance of the AgglomerativeClustering class\n",
        "                ac = AgglomerativeClustering(n_clusters=None,metric=affinity, linkage=linkage, distance_threshold=distance_threshold)\n",
        "                \n",
        "                # fit the data and get the labels\n",
        "                ac.fit(data)\n",
        "                labels = ac.labels_\n",
        "                \n",
        "                # compute the silhouette score\n",
        "                if distance_threshold ==0:\n",
        "                    # use the default number of clusters (2)\n",
        "                    score = silhouette_score(data, labels, metric=affinity)\n",
        "                else:\n",
        "                    # use the number of clusters found by the algorithm\n",
        "                    score = silhouette_score(data, labels, metric=affinity)\n",
        "                \n",
        "                sil_scores_data2[counter].append(score)\n",
        "                corres_params_data2[counter].append({'Affinity':affinity,'Linkage':linkage,'Threshold':distance_threshold})\n",
        "                \n",
        "                # Perform PCA to reduce the data to 2 dimensions\n",
        "                pca = PCA(n_components=2, random_state=0).fit(data)\n",
        "                data_2d = pca.transform(data)\n",
        "                \n",
        "                # plot the clusters\n",
        "                fig2 = plt.figure(figsize=(10, 5))\n",
        "                plt.title(f'Affinity: {affinity}, Linkage: {linkage}, Distance Threshold: {distance_threshold}, silhouette score: {score}')\n",
        "                plt.xlabel(f'Number of Clusters: {len(np.unique(labels))}')\n",
        "                plt.ylabel(f\"{desc}\")\n",
        "                plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "                plt.show()\n",
        "                display_clusters(data,ac)\n",
        "    counter+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the range of parameters\n",
        "eps_values = np.arange(.6, 1.6, 0.1)\n",
        "min_samples_values = np.arange(2, 13, 1)\n",
        "\n",
        "# initialize an empty list to store the silhouette scores\n",
        "sil_scores_data3 = [[],[],[],[]]\n",
        "corres_params_data3  = [[],[],[],[]]\n",
        "counter = 0\n",
        "for i in data_points_list:\n",
        "    if counter!=1:\n",
        "        counter+=1\n",
        "        continue\n",
        "    data = i[\"data\"]\n",
        "    desc = i[\"desc\"]\n",
        "    print(f\"graphs for {desc}\")\n",
        "    \n",
        "    # loop over the combinations of parameters\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            # create an instance of the DBSCAN class\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            \n",
        "            # fit the data and get the labels\n",
        "            dbscan.fit(data)\n",
        "            labels = dbscan.labels_\n",
        "            \n",
        "            # compute the silhouette score\n",
        "            # ignore the noise points (labelled as -1) for the score calculation\n",
        "            score = silhouette_score(data, labels)\n",
        "            \n",
        "            # append the score to the list\n",
        "            sil_scores_data3[counter].append(score)\n",
        "            corres_params_data3[counter].append({\"eps\":eps, \"min_samples\":min_samples})\n",
        "            # Perform PCA to reduce the data to 2 dimensions\n",
        "            pca = PCA(n_components=2, random_state=0).fit(data)\n",
        "            data_2d = pca.transform(data)\n",
        "            # plot the clusters\n",
        "            fig = plt.figure(figsize=(10, 5))\n",
        "            plt.title(f'EPS: {eps}, Min_samples: {min_samples}, silhouette score: {score}')\n",
        "            plt.xlabel(f'Number of Clusters: {len(np.unique(labels)) - 1}') # subtract 1 to exclude the noise cluster\n",
        "            plt.ylabel(f'{desc}')\n",
        "            plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='rainbow',alpha=0.5,s=20)\n",
        "            plt.show()\n",
        "            display_clusters(data,dbscan)\n",
        "    counter+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = 0\n",
        "for X in data_points_list:\n",
        "    data = X[\"data\"]\n",
        "    desc = X[\"desc\"]\n",
        "    print(f\"graphs for {desc}\")\n",
        "    # Apply PCA to reduce the dimensionality to 2\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X['data'])\n",
        "    \n",
        "    # Define a list of covariance types to try\n",
        "    cov_types = ['full','spherical', 'diag', 'tied']\n",
        "\n",
        "    # Loop over the covariance types and fit a GMM for each one\n",
        "    for cov_type in cov_types:\n",
        "        gmm = GaussianMixture(n_components=5, covariance_type=cov_type, random_state=42)\n",
        "        gmm.fit(X_pca)\n",
        "        y_pred = gmm.predict(X_pca)\n",
        "        # Display the clusters\n",
        "        display_cluster(X_pca, y_pred, f'GMM with 5 components and {cov_type} covariance')\n",
        "        plot_gmm_contours(gmm, X_pca, f'GMM with 5 components and {cov_type} covariance')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Clustering Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
